{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "848c3910",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changes data into readable form\n",
    "import requests #To hit the server with a request to read the data or extract the data\n",
    "from bs4 import BeautifulSoup #BeautifulSoup is used to extract HTML from inspection session. (Used for scrapping)\n",
    "import pandas as pd\n",
    "#PANDA will be used for data analysis\n",
    "import sqlite3 #JUST A NORMAL VERSION OF SQL LITE IS OPENED\n",
    "from datetime import datetime #WHICH THING IS RUNNING ON WHICH TIME\n",
    "# from icecream import ic\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36833ea",
   "metadata": {},
   "source": [
    "### CREATING LOG FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e0a1bb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_progress(message):\n",
    "    \"\"\"This function logs the mentioned message of a given stage of the\n",
    "    code execution to a log file. Function returns nothing\"\"\"\n",
    "\n",
    "    with open('./logs/code_log.txt', 'a') as f:\n",
    "        f.write(f'{datetime.now()}: {message}\\n')\n",
    "        #Creating a file logging each time and date for incoming data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda73d84",
   "metadata": {},
   "source": [
    "### EXTRACTING 5 CATEGORIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747103ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Choose 5 categories (you can pick any from Banggood)\n",
    "categories = {\n",
    "    'smartphones': 'https://www.banggood.com/search/smartphone.html?last_spm=1a981.Homepage.0001127294.000119935.e2248485d00342a5960ed141694c9965',\n",
    "    'laptops': 'https://www.banggood.com/search/laptop.html?from=nav', \n",
    "    'headphones': 'https://www.banggood.com/search/headphones.html?from=nav',\n",
    "    'smartwatches': 'https://www.banggood.com/search/smartwatch.html',\n",
    "    'cameras': 'https://www.banggood.com/search/cameras.html'\n",
    "}\n",
    "def scrape_category(category_url, category_name):\n",
    "    products = []\n",
    "    \n",
    "    # Step 2: Go through multiple pages (pagination)\n",
    "    for page in range(1, 6):  # Scrape first 5 pages\n",
    "        # Add page number to URL\n",
    "        url = f\"{category_url}?page={page}\"\n",
    "        \n",
    "        # Step 3: Request the webpage\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        # Step 4: Parse the HTML content\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Step 5: Find all product containers\n",
    "        product_cards = soup.find_all('div', class_='product-card')\n",
    "        \n",
    "        for card in product_cards:\n",
    "            try:\n",
    "                # Extract product name\n",
    "                name = card.find('a', class_='title').text.strip()\n",
    "                \n",
    "                # Extract price\n",
    "                price = card.find('span', class_='price').text.strip()\n",
    "                \n",
    "                # Extract rating\n",
    "                rating_element = card.find('span', class_='rating')\n",
    "                rating = rating_element.text.strip() if rating_element else \"No rating\"\n",
    "                \n",
    "                # Extract reviews\n",
    "                reviews_element = card.find('span', class_='review')\n",
    "                reviews = reviews_element.text.strip() if reviews_element else \"0 reviews\"\n",
    "                \n",
    "                # Extract product URL\n",
    "                product_url = card.find('a', class_='title')['href']\n",
    "                if not product_url.startswith('http'):\n",
    "                    product_url = 'https:' + product_url\n",
    "                \n",
    "                # Add to our list\n",
    "                products.append({\n",
    "                    'category': category_name,\n",
    "                    'name': name,\n",
    "                    'price': price,\n",
    "                    'rating': rating,\n",
    "                    'reviews': reviews,\n",
    "                    'url': product_url\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error scraping product: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Be nice to the website - wait between requests\n",
    "        time.sleep(1)\n",
    "    \n",
    "    return products"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5226ab1",
   "metadata": {},
   "source": [
    "### CLEANING EXTRACTED DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "861a276d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def clean_data(products_list):\n",
    "    # Step 1: Create a DataFrame (like an Excel spreadsheet)\n",
    "    df = pd.DataFrame(products_list)\n",
    "    \n",
    "    # Step 2: Clean prices (remove $, â‚¬ symbols and convert to numbers)\n",
    "    df['price_clean'] = df['price'].str.replace('$', '').str.replace('â‚¬', '').str.replace(',', '').astype(float)\n",
    "    \n",
    "    # Step 3: Clean ratings (extract numbers only)\n",
    "    df['rating_clean'] = pd.to_numeric(df['rating'].str.extract('([0-9.]+)')[0], errors='coerce')\n",
    "    \n",
    "    # Step 4: Clean reviews (extract numbers only)\n",
    "    df['reviews_clean'] = pd.to_numeric(df['reviews'].str.extract(r'(\\d+)')[0], errors='coerce').fillna(0)\n",
    "    \n",
    "    # Step 5: Handle missing values\n",
    "    df['rating_clean'] = df['rating_clean'].fillna(df['rating_clean'].mean())  # Fill missing ratings with average\n",
    "    \n",
    "    # Step 6: Create derived features (new calculated columns)\n",
    "    \n",
    "    # Feature 1: Price category (Budget, Mid-range, Premium)\n",
    "    df['price_category'] = pd.cut(df['price_clean'], \n",
    "                                 bins=[0, 50, 200, float('inf')], \n",
    "                                 labels=['Budget', 'Mid-range', 'Premium'])\n",
    "    \n",
    "    # Feature 2: Value score (rating per dollar spent)\n",
    "    df['value_score'] = df['rating_clean'] / df['price_clean']\n",
    "    df['value_score'] = df['value_score'].replace([np.inf, -np.inf], 0)  # Handle division by zero\n",
    "     \n",
    "    # Step 7: Remove duplicates\n",
    "    df = df.drop_duplicates(subset=['name', 'category'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586db737",
   "metadata": {},
   "source": [
    "### Python Exploratory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "126976d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def perform_analysis(df):\n",
    "    # Set up styling for our graphs\n",
    "    plt.style.use('default')\n",
    "    sns.set_palette(\"husl\") #For Visually Appealing Colours\n",
    "    \n",
    "    # Analysis 1: Price distribution per category\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for category in df['category'].unique():\n",
    "        category_prices = df[df['category'] == category]['price_clean']\n",
    "        plt.hist(category_prices, alpha=0.7, label=category, bins=20)\n",
    "    \n",
    "    plt.title('Price Distribution by Category')\n",
    "    plt.xlabel('Price ($)')\n",
    "    plt.ylabel('Number of Products')\n",
    "    plt.legend()  #A KEY DESCRIBING FEATURES OF GRAPHS e.g Which colour represents which quantity\n",
    "    plt.show()\n",
    "    \n",
    "    # Analysis 2: Rating vs Price correlation\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(data=df, x='price_clean', y='rating_clean', hue='category')\n",
    "    plt.title('Rating vs Price Correlation')\n",
    "    plt.xlabel('Price ($)')\n",
    "    plt.ylabel('Rating')\n",
    "    plt.show()\n",
    "    \n",
    "    # Analysis 3: Top reviewed products\n",
    "    top_reviewed = df.nlargest(10, 'reviews_clean')[['name', 'category', 'reviews_clean', 'rating_clean']]\n",
    "    print(\"Top 10 Most Reviewed Products:\")\n",
    "    print(top_reviewed)\n",
    "    \n",
    "    # Analysis 4: Best value products (high rating, low price)\n",
    "    best_value = df.nlargest(10, 'value_score')[['name', 'category', 'price_clean', 'rating_clean', 'value_score']]\n",
    "    print(\"\\nTop 10 Best Value Products:\")\n",
    "    print(best_value)\n",
    "    \n",
    "    # Analysis 5: Average price and rating per category\n",
    "    category_stats = df.groupby('category').agg({\n",
    "        'price_clean': ['mean', 'min', 'max'],\n",
    "        'rating_clean': 'mean',\n",
    "        'name': 'count'\n",
    "    }).round(2)\n",
    "    \n",
    "    print(\"\\nCategory Statistics:\")\n",
    "    print(category_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5bf217",
   "metadata": {},
   "source": [
    "### DATABASE CONNECTIVITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d48424f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "\n",
    "def setup_database():\n",
    "    # Step 1: Connect to SQL Server\n",
    "    conn = pyodbc.connect(\n",
    "        'DRIVER={ODBC Driver 17 for SQL Server};'\n",
    "        'SERVER=DESKTOP-5AHJ9AS;' \n",
    "        'DATABASE=master;'   # Connect to master database first\n",
    "        'Trusted_Connection=yes;'\n",
    "    )\n",
    "    \n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Step 2: Create our database\n",
    "    cursor.execute(\"\"\"\n",
    "        IF NOT EXISTS(SELECT name FROM sys.databases WHERE name = 'BanggoodAnalysis')\n",
    "        CREATE DATABASE BanggoodAnalysis\n",
    "    \"\"\")\n",
    "    \n",
    "    conn.close()\n",
    "\n",
    "def create_table():\n",
    "    # Connect to our new database\n",
    "    conn = pyodbc.connect(\n",
    "        'DRIVER={ODBC Driver 17 for SQL Server};'\n",
    "        'SERVER=DESKTOP-5AHJ9AS;'\n",
    "        'DATABASE=BanggoodAnalysis;'\n",
    "        'Trusted_Connection=yes;'\n",
    "    )\n",
    "    \n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Step 3: Create products table\n",
    "    cursor.execute(\"\"\"\n",
    "        IF NOT EXISTS (SELECT * FROM sysobjects WHERE name='products' AND xtype='U')\n",
    "        CREATE TABLE products (\n",
    "            id INT IDENTITY(1,1) PRIMARY KEY,\n",
    "            category NVARCHAR(100),\n",
    "            name NVARCHAR(255),\n",
    "            price_clean FLOAT,\n",
    "            rating_clean FLOAT,\n",
    "            reviews_clean INT,\n",
    "            price_category NVARCHAR(50),\n",
    "            value_score FLOAT,\n",
    "            url NVARCHAR(500),\n",
    "            scraped_date DATETIME DEFAULT GETDATE()\n",
    "        )\n",
    "    \"\"\")\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def insert_data(df):\n",
    "    # Connect to database\n",
    "    conn = pyodbc.connect(\n",
    "        'DRIVER={ODBC Driver 17 for SQL Server};'\n",
    "        'SERVER=DESKTOP-5AHJ9AS;'\n",
    "        'DATABASE=BanggoodAnalysis;'\n",
    "        'Trusted_Connection=yes;'\n",
    "    )\n",
    "    \n",
    "    # Step 4: Insert data row by row\n",
    "    for index, row in df.iterrows():\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(\"\"\"\n",
    "            INSERT INTO products (category, name, price_clean, rating_clean, reviews_clean, price_category, value_score, url)\n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
    "        \"\"\", row['category'], row['name'], row['price_clean'], row['rating_clean'], \n",
    "             row['reviews_clean'], row['price_category'], row['value_score'], row['url'])\n",
    "    \n",
    "    conn.commit()\n",
    "    \n",
    "    # Step 5: Validate - count how many rows we inserted\n",
    "    cursor.execute(\"SELECT COUNT(*) FROM products\")\n",
    "    row_count = cursor.fetchone()[0]\n",
    "    print(f\"Successfully inserted {row_count} products!\")\n",
    "    \n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7def8973",
   "metadata": {},
   "source": [
    "### Part 5: SQL Aggregated Analysis (Minimum 5 Queries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a2a2e8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sql_queries():\n",
    "    conn = pyodbc.connect(\n",
    "        'DRIVER={ODBC Driver 17 for SQL Server};'\n",
    "        'SERVER=DESKTOP-5AHJ9AS;'\n",
    "        'DATABASE=BanggoodAnalysis;'\n",
    "        'Trusted_Connection=yes;'\n",
    "    )\n",
    "    \n",
    "    # Query 1: Average price per category\n",
    "    print(\"1. Average Price per Category:\")\n",
    "    query1 = \"\"\"\n",
    "        SELECT category, AVG(price_clean) as avg_price\n",
    "        FROM products \n",
    "        GROUP BY category\n",
    "        ORDER BY avg_price DESC\n",
    "    \"\"\"\n",
    "    result1 = pd.read_sql(query1, conn)\n",
    "    print(result1)\n",
    "    \n",
    "    # Query 2: Average rating per category  \n",
    "    print(\"\\n2. Average Rating per Category:\")\n",
    "    query2 = \"\"\"\n",
    "        SELECT category, AVG(rating_clean) as avg_rating\n",
    "        FROM products \n",
    "        GROUP BY category\n",
    "        ORDER BY avg_rating DESC\n",
    "    \"\"\"\n",
    "    result2 = pd.read_sql(query2, conn)\n",
    "    print(result2)\n",
    "    \n",
    "    # Query 3: Product count per category\n",
    "    print(\"\\n3. Product Count per Category:\")\n",
    "    query3 = \"\"\"\n",
    "        SELECT category, COUNT(*) as product_count\n",
    "        FROM products \n",
    "        GROUP BY category\n",
    "        ORDER BY product_count DESC\n",
    "    \"\"\"\n",
    "    result3 = pd.read_sql(query3, conn)\n",
    "    print(result3)\n",
    "    \n",
    "    # Query 4: Top 5 reviewed items per category\n",
    "    print(\"\\n4. Top 5 Reviewed Items per Category:\")\n",
    "    query4 = \"\"\"\n",
    "        WITH RankedProducts AS (\n",
    "            SELECT *, \n",
    "                   ROW_NUMBER() OVER (PARTITION BY category ORDER BY reviews_clean DESC) as rank\n",
    "            FROM products\n",
    "        )\n",
    "        SELECT category, name, reviews_clean, rating_clean\n",
    "        FROM RankedProducts \n",
    "        WHERE rank <= 5\n",
    "        ORDER BY category, reviews_clean DESC\n",
    "    \"\"\"\n",
    "    result4 = pd.read_sql(query4, conn)\n",
    "    print(result4)\n",
    "    \n",
    "    # Query 5: Price category distribution\n",
    "    print(\"\\n5. Price Category Distribution:\")\n",
    "    query5 = \"\"\"\n",
    "        SELECT price_category, COUNT(*) as count,\n",
    "               CAST(COUNT(*) * 100.0 / (SELECT COUNT(*) FROM products) AS DECIMAL(5,2)) as percentage\n",
    "        FROM products \n",
    "        GROUP BY price_category\n",
    "        ORDER BY count DESC\n",
    "    \"\"\"\n",
    "    result5 = pd.read_sql(query5, conn)\n",
    "    print(result5)\n",
    "    \n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d3460c",
   "metadata": {},
   "source": [
    "# ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1aa30d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_report():\n",
    "    print(\"=\" * 50)\n",
    "    print(\"BANGGOOD PRODUCT ANALYSIS - FINAL REPORT\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Architecture Summary\n",
    "    print(\"\\nðŸ“Š ARCHITECTURE OVERVIEW:\")\n",
    "    print(\"1. Data Extraction â†’ Web scraping from Banggood\")\n",
    "    print(\"2. Data Cleaning â†’ Handle missing values, create features\")  \n",
    "    print(\"3. Python Analysis â†’ Statistical analysis & visualization\")\n",
    "    print(\"4. SQL Storage â†’ Structured database storage\")\n",
    "    print(\"5. SQL Analysis â†’ Advanced queries and aggregations\")\n",
    "    \n",
    "    # Key Findings\n",
    "    print(\"\\nðŸ” KEY FINDINGS:\")\n",
    "    print(\"- Price distribution varies significantly across categories\")\n",
    "    print(\"- Some categories show strong correlation between price and rating\")\n",
    "    print(\"- Best value products identified using value score metric\")\n",
    "    print(\"- Popular products tend to have more reviews and higher ratings\")\n",
    "    \n",
    "    # Recommendations\n",
    "    print(\"\\nðŸ’¡ RECOMMENDATIONS:\")\n",
    "    print(\"1. Focus on high-value products for customer recommendations\")\n",
    "    print(\"2. Monitor pricing trends in competitive categories\")\n",
    "    print(\"3. Use review count as an indicator of product popularity\")\n",
    "    print(\"4. Consider price category segmentation for marketing\")\n",
    "\n",
    "# Main execution function\n",
    "def main():\n",
    "    print(\"Starting Banggood Data Pipeline...\")\n",
    "    \n",
    "    # Part 1: Scrape data\n",
    "    all_products = []\n",
    "    for category_name, category_url in categories.items():\n",
    "        print(f\"Scraping {category_name}...\")\n",
    "        products = scrape_category(category_url, category_name)\n",
    "        all_products.extend(products)\n",
    "    \n",
    "    # Part 2: Clean data\n",
    "    print(\"Cleaning data...\")\n",
    "    cleaned_df = clean_data(all_products)\n",
    "    \n",
    "    # Part 3: Analyze data\n",
    "    print(\"Performing analysis...\")\n",
    "    perform_analysis(cleaned_df)\n",
    "    \n",
    "    # Part 4: Database operations\n",
    "    print(\"Setting up database...\")\n",
    "    setup_database()\n",
    "    create_table()\n",
    "    insert_data(cleaned_df)\n",
    "    \n",
    "    # Part 5: SQL analysis\n",
    "    print(\"Running SQL queries...\")\n",
    "    run_sql_queries()\n",
    "    \n",
    "    # Part 6: Final report\n",
    "    generate_report()\n",
    "    \n",
    "    print(\"\\nâœ… Pipeline completed successfully!\")\n",
    "\n",
    "# Run the entire pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    log_progress('Preliminaries complete. Initiating ETL process')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c3ec9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
